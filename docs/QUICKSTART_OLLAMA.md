# ðŸš€ Ollama Quick Start - 5 Minuten Setup

## Schritt 1: Services starten

```bash
cd eki-api
docker compose up -d
```

**Warten Sie ~30 Sekunden, bis alle Services bereit sind.**

## Schritt 2: Ollama-Modell herunterladen

```bash
# Mistral herunterladen (~4GB, dauert 2-5 Min je nach Internet)
docker exec -it eki-ollama ollama pull mistral
```

**Fortschritt wird angezeigt:**
```
pulling manifest
pulling 61e88e884507... 100%
...
success
```

## Schritt 3: Provider konfigurieren

```bash
# .env bearbeiten oder diese Defaults nutzen:
echo "LLM_PROVIDER=ollama" >> .env
echo "OLLAMA_MODEL=mistral" >> .env

# API und Worker neu starten
docker compose restart api worker
```

## Schritt 4: Testen! ðŸŽ‰

```bash
docker compose exec api python scripts/test_llm.py
```

**Erfolgreiche Ausgabe:**
```
ðŸ”§ Testing LLM Provider: ollama
============================================================
âœ… Provider initialized: ollama
ðŸ¥ Running health check...
âœ… Provider is healthy
ðŸ¤– Testing text generation...
Response: Hello from eKI!
ðŸ¤– Testing with system prompt...
Response: 4
ðŸ“Š Testing structured generation...
Structured response: {'answer': 'Yes, the sky is blue', 'confidence': 0.95}
ðŸ“‹ Available Ollama models:
  - mistral:latest
âœ… All tests passed!
```

## Alternative: Mistral Cloud nutzen

Wenn Sie lieber Mistral Cloud verwenden:

```bash
# .env bearbeiten
echo "LLM_PROVIDER=mistral_cloud" >> .env
echo "MISTRAL_API_KEY=your-api-key" >> .env

# Ollama kann auskommentiert werden in docker-compose.yml
# Services neu starten
docker compose restart api worker

# Testen
docker compose exec api python scripts/test_llm.py
```

## Provider wechseln (jederzeit)

```bash
# Einfach .env Ã¤ndern:
nano .env
# LLM_PROVIDER=ollama â†’ LLM_PROVIDER=mistral_cloud

# Neu starten
docker compose restart api worker
```

## Troubleshooting

### "Model not found"
```bash
docker exec -it eki-ollama ollama list
# Wenn leer: ollama pull mistral
```

### "Connection refused"
```bash
docker compose ps ollama
# Sollte "Up" sein

docker compose logs ollama
# Logs prÃ¼fen
```

### Ollama ist langsam (CPU)
Das ist normal ohne GPU. FÃ¼r Produktion:
- GPU-Server verwenden (10-20x schneller)
- Oder Mistral Cloud nutzen

## Fertig! ðŸŽ‰

Sie kÃ¶nnen jetzt:
- âœ… Zwischen Ollama und Mistral Cloud wechseln
- âœ… Verschiedene Modelle testen
- âœ… Die API mit echten LLMs entwickeln

Weitere Infos:
- ðŸ“– [docs/OLLAMA_SETUP.md](docs/OLLAMA_SETUP.md) - AusfÃ¼hrliche Anleitung
- ðŸ“– [docs/LLM_INTEGRATION.md](docs/LLM_INTEGRATION.md) - API-Dokumentation
- ðŸ”§ [scripts/test_llm.py](scripts/test_llm.py) - Test-Script
