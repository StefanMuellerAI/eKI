#!/usr/bin/env python3
"""Test script for LLM providers."""

import asyncio
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from api.config import get_settings
from llm.factory import get_llm_provider


async def main() -> None:
    """Test LLM provider."""
    settings = get_settings()

    print(f"ğŸ”§ Testing LLM Provider: {settings.llm_provider}")
    print("=" * 60)

    try:
        # Create provider
        provider = get_llm_provider(settings)
        print(f"âœ… Provider initialized: {provider.provider_name}")

        # Health check
        print("\nğŸ¥ Running health check...")
        is_healthy = await provider.health_check()
        if is_healthy:
            print("âœ… Provider is healthy")
        else:
            print("âŒ Provider health check failed")
            return

        # Test simple generation
        print("\nğŸ¤– Testing text generation...")
        response = await provider.generate(
            prompt="Say 'Hello from eKI!' and nothing else.",
            temperature=0.1,
            max_tokens=50,
        )
        print(f"Response: {response}")

        # Test with system prompt
        print("\nğŸ¤– Testing with system prompt...")
        response = await provider.generate(
            prompt="What is 2+2?",
            system_prompt="You are a helpful math assistant. Answer concisely.",
            temperature=0.1,
            max_tokens=50,
        )
        print(f"Response: {response}")

        # Test structured generation
        print("\nğŸ“Š Testing structured generation...")
        schema = {
            "type": "object",
            "properties": {"answer": {"type": "string"}, "confidence": {"type": "number"}},
            "required": ["answer", "confidence"],
        }

        structured_response = await provider.generate_structured(
            prompt="Is the sky blue? Answer with confidence 0-1.",
            schema=schema,
            temperature=0.1,
        )
        print(f"Structured response: {structured_response}")

        # If Ollama, show available models
        if provider.provider_name == "ollama":
            print("\nğŸ“‹ Available Ollama models:")
            models = await provider.list_models()
            if models:
                for model in models:
                    print(f"  - {model}")
            else:
                print("  No models found. Pull a model first:")
                print("  docker exec -it eki-ollama ollama pull mistral")

        print("\nâœ… All tests passed!")

    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
